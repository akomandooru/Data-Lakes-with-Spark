{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import shutil\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load config info\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load song data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o67.json.\n: java.io.IOException: No FileSystem for scheme: s3\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2660)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:547)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:545)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:355)\n\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:545)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:359)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:391)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-85f02d66e02c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m's3://udacity-dend/song_data/*/*/Z/*.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding)\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o67.json.\n: java.io.IOException: No FileSystem for scheme: s3\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2660)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:547)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:545)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:355)\n\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:545)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:359)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:391)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json('s3://udacity-dend/song_data/*/*/Z/*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a new view to select song data and write to parquet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+------------------+----+---------+\n",
      "|           song_id|               title|         artist_id|year| duration|\n",
      "+------------------+--------------------+------------------+----+---------+\n",
      "|SOYMRWW12A6D4FAB14|The Moon And I (O...|ARKFYS91187B98E58F|   0| 267.7024|\n",
      "|SOHKNRJ12A6701D1F8|        Drop of Rain|AR10USD1187B99F3F1|   0|189.57016|\n",
      "|SOQHXMF12AB0182363|     Young Boy Blues|ARGSJW91187B9B1D6B|   0|218.77506|\n",
      "|SOCIWDW12A8C13D406|           Soul Deep|ARMJAGH1187FB546F3|1969|148.03546|\n",
      "|SONHOTT12A8C13493C|     Something Girls|AR7G5I41187FB4CE6C|1982|233.40363|\n",
      "|SOMZWCG12A8C13C480|    I Didn't Mean To|ARD7TVE1187B99BFB1|   0|218.93179|\n",
      "|SOUDSGM12AC9618304|Insatiable (Instr...|ARNTLGG11E2835DDB9|   0|266.39628|\n",
      "|SOMJBYD12A6D4F8557|Keepin It Real (S...|ARD0S291187B9B7BF5|   0|114.78159|\n",
      "|SOXVLOJ12AB0189215|     Amor De Cabaret|ARKRRTF1187B9984DA|   0|177.47546|\n",
      "|SOIAZJW12AB01853F1|          Pink World|AR8ZCNI1187B9A069B|1984|269.81832|\n",
      "|SOFSOCN12A8C143F5D|      Face the Ashes|ARXR32B1187FB57099|2007|209.60608|\n",
      "+------------------+--------------------+------------------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"song_data\")\n",
    "songs_table = spark.sql(\"\"\"\n",
    "    SELECT song_id, title, artist_id, year, duration\n",
    "    FROM song_data\n",
    "\"\"\")\n",
    "import shutil\n",
    "shutil.rmtree(\"./data/output/songs_table\", ignore_errors=True)\n",
    "\n",
    "songs_table.write.partitionBy(\"year\", \"artist_id\").parquet(\"./data/output/songs_table\")\n",
    "songs_table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write artist table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+--------------------+---------------+----------------+\n",
      "|         artist_id|         artist_name|     artist_location|artist_latitude|artist_longitude|\n",
      "+------------------+--------------------+--------------------+---------------+----------------+\n",
      "|ARNTLGG11E2835DDB9|                 Clp|                    |           null|            null|\n",
      "|ARKRRTF1187B9984DA|    Sonora Santanera|                    |           null|            null|\n",
      "|AR10USD1187B99F3F1|Tweeterfriendly M...|Burlington, Ontar...|           null|            null|\n",
      "|AR7G5I41187FB4CE6C|            Adam Ant|     London, England|           null|            null|\n",
      "|ARKFYS91187B98E58F|Jeff And Sheri Ea...|                    |           null|            null|\n",
      "|AR8ZCNI1187B9A069B|    Planet P Project|                    |           null|            null|\n",
      "|ARGSJW91187B9B1D6B|        JennyAnyKind|      North Carolina|       35.21962|       -80.01955|\n",
      "|ARD0S291187B9B7BF5|             Rated R|                Ohio|           null|            null|\n",
      "|ARMJAGH1187FB546F3|        The Box Tops|         Memphis, TN|       35.14968|       -90.04892|\n",
      "|ARXR32B1187FB57099|                 Gob|                    |           null|            null|\n",
      "|ARD7TVE1187B99BFB1|              Casual|     California - LA|           null|            null|\n",
      "+------------------+--------------------+--------------------+---------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "artists_table = spark.sql(\"\"\"\n",
    "    SELECT artist_id, min(artist_name) as artist_name, min(artist_location) as artist_location, \n",
    "    min(artist_latitude) as artist_latitude, min(artist_longitude) as artist_longitude\n",
    "    FROM song_data GROUP BY artist_id\"\"\")\n",
    "shutil.rmtree(\"./data/output/artists_table\", ignore_errors=True)\n",
    "artists_table.write.parquet(\"./data/output/artists_table\")\n",
    "artists_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_data = \"./data/\" + \"log_data/*/*/*.json\"\n",
    "dfl = spark.read.json(log_data)\n",
    "dfl.createOrReplaceTempView(\"log_data\")\n",
    "dfl = spark.sql(\"\"\"SELECT * FROM log_data WHERE page='NextSong'\"\"\")\n",
    "dfl.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+------+-----+\n",
      "|user_id|first_name|last_name|gender|level|\n",
      "+-------+----------+---------+------+-----+\n",
      "|     51|      Maia|    Burke|     F| free|\n",
      "|      7|    Adelyn|   Jordan|     F| free|\n",
      "|     15|      Lily|     Koch|     F| free|\n",
      "|     54|     Kaleb|     Cook|     M| free|\n",
      "|    101|    Jayden|      Fox|     M| free|\n",
      "|     11| Christian|   Porter|     F| free|\n",
      "|     29|Jacqueline|    Lynch|     F| free|\n",
      "|     69|  Anabelle|  Simpson|     F| free|\n",
      "|     42|    Harper|  Barrett|     M| paid|\n",
      "|     73|     Jacob|    Klein|     M| paid|\n",
      "|     64|    Hannah|  Calhoun|     F| free|\n",
      "|      3|     Isaac|   Valdez|     M| free|\n",
      "|     30|     Avery|  Watkins|     F| paid|\n",
      "|     34|    Evelin|    Ayala|     F| free|\n",
      "|     59|      Lily|   Cooper|     F| free|\n",
      "|      8|    Kaylee|  Summers|     F| free|\n",
      "|     28|  Brantley|     West|     M| free|\n",
      "|     85|   Kinsley|    Young|     F| free|\n",
      "|     16|     Rylan|   George|     M| free|\n",
      "|     35|     Molly|   Taylor|     F| free|\n",
      "+-------+----------+---------+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_table = spark.sql(\"\"\"\n",
    "    SELECT userId as user_id, min(firstName) as first_name, min(lastName) as last_name, min(gender) as gender, min(level) as level\n",
    "    FROM log_data GROUP BY userId\n",
    "\"\"\")\n",
    "shutil.rmtree(\"./data/output/users_table\",ignore_errors=True)\n",
    "users_table.write.parquet(\"./data/output/users_table\")\n",
    "users_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  pyspark.sql.functions as F\n",
    "dfl = dfl.withColumn('timestamp', F.to_timestamp(dfl['ts']/1000))\n",
    "dfl = dfl.withColumn('datetime', F.to_date('timestamp'))\n",
    "dfl.createOrReplaceTempView(\"log_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+---+----+-----+----+-------+\n",
      "|   start_time|hour|day|week|month|year|weekday|\n",
      "+-------------+----+---+----+-----+----+-------+\n",
      "|1542241826796|   0| 15|  46|   11|2018|      5|\n",
      "|1542242481796|   0| 15|  46|   11|2018|      5|\n",
      "|1542242741796|   0| 15|  46|   11|2018|      5|\n",
      "|1542253449796|   3| 15|  46|   11|2018|      5|\n",
      "|1542260935796|   5| 15|  46|   11|2018|      5|\n",
      "|1542261224796|   5| 15|  46|   11|2018|      5|\n",
      "|1542261356796|   5| 15|  46|   11|2018|      5|\n",
      "|1542261662796|   6| 15|  46|   11|2018|      5|\n",
      "|1542262057796|   6| 15|  46|   11|2018|      5|\n",
      "|1542262233796|   6| 15|  46|   11|2018|      5|\n",
      "|1542262434796|   6| 15|  46|   11|2018|      5|\n",
      "|1542262456796|   6| 15|  46|   11|2018|      5|\n",
      "|1542262679796|   6| 15|  46|   11|2018|      5|\n",
      "|1542262728796|   6| 15|  46|   11|2018|      5|\n",
      "|1542262893796|   6| 15|  46|   11|2018|      5|\n",
      "|1542263158796|   6| 15|  46|   11|2018|      5|\n",
      "|1542263378796|   6| 15|  46|   11|2018|      5|\n",
      "|1542265716796|   7| 15|  46|   11|2018|      5|\n",
      "|1542265929796|   7| 15|  46|   11|2018|      5|\n",
      "|1542266927796|   7| 15|  46|   11|2018|      5|\n",
      "+-------------+----+---+----+-----+----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format, dayofweek\n",
    "time_table = spark.sql(\"\"\"\n",
    "    SELECT ts as start_time, hour(timestamp) as hour, dayofmonth(timestamp) as day, weekofyear(timestamp) as week,\n",
    "    month(timestamp) as month, year(timestamp) as year, dayofweek(timestamp) as weekday\n",
    "    FROM log_data\n",
    "\"\"\")\n",
    "shutil.rmtree(\"./data/output/time_table\",ignore_errors=True)\n",
    "time_table.write.partitionBy(\"year\", \"month\").parquet(\"./data/output/time_table\")\n",
    "time_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+--------------------+---------------+----------------+\n",
      "|         artist_id|         artist_name|     artist_location|artist_latitude|artist_longitude|\n",
      "+------------------+--------------------+--------------------+---------------+----------------+\n",
      "|AR10USD1187B99F3F1|Tweeterfriendly M...|Burlington, Ontar...|           null|            null|\n",
      "|ARGSJW91187B9B1D6B|        JennyAnyKind|      North Carolina|       35.21962|       -80.01955|\n",
      "|ARMJAGH1187FB546F3|        The Box Tops|         Memphis, TN|       35.14968|       -90.04892|\n",
      "|AR7G5I41187FB4CE6C|            Adam Ant|     London, England|           null|            null|\n",
      "|ARD7TVE1187B99BFB1|              Casual|     California - LA|           null|            null|\n",
      "|ARKFYS91187B98E58F|Jeff And Sheri Ea...|                    |           null|            null|\n",
      "|AR8ZCNI1187B9A069B|    Planet P Project|                    |           null|            null|\n",
      "|ARKRRTF1187B9984DA|    Sonora Santanera|                    |           null|            null|\n",
      "|ARD0S291187B9B7BF5|             Rated R|                Ohio|           null|            null|\n",
      "|ARNTLGG11E2835DDB9|                 Clp|                    |           null|            null|\n",
      "|ARXR32B1187FB57099|                 Gob|                    |           null|            null|\n",
      "+------------------+--------------------+--------------------+---------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_data = \"./data/output/\"\n",
    "song_df = spark.read.parquet(output_data + \"songs_table\")\n",
    "song_df.createOrReplaceTempView(\"song_view\")\n",
    "artist_df = spark.read.parquet(output_date + \"artists_table\")\n",
    "artist_df.createOrReplaceTempView(\"artist_view\")\n",
    "artist_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artist_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+-----+-------+---------+----------+--------------------+--------------------+\n",
      "|songplay_id|   start_time|user_id|level|song_id|artist_id|session_id|            location|          user_agent|\n",
      "+-----------+-------------+-------+-----+-------+---------+----------+--------------------+--------------------+\n",
      "|          0|1542241826796|     26| free|   null|     null|       583|San Jose-Sunnyval...|\"Mozilla/5.0 (X11...|\n",
      "|          1|1542242481796|     26| free|   null|     null|       583|San Jose-Sunnyval...|\"Mozilla/5.0 (X11...|\n",
      "|          2|1542242741796|     26| free|   null|     null|       583|San Jose-Sunnyval...|\"Mozilla/5.0 (X11...|\n",
      "|          3|1542253449796|     61| free|   null|     null|       597|Houston-The Woodl...|\"Mozilla/5.0 (Mac...|\n",
      "|          4|1542260935796|     80| paid|   null|     null|       602|Portland-South Po...|\"Mozilla/5.0 (Mac...|\n",
      "|          5|1542261224796|     80| paid|   null|     null|       602|Portland-South Po...|\"Mozilla/5.0 (Mac...|\n",
      "|          6|1542261356796|     80| paid|   null|     null|       602|Portland-South Po...|\"Mozilla/5.0 (Mac...|\n",
      "|          7|1542261662796|     80| paid|   null|     null|       602|Portland-South Po...|\"Mozilla/5.0 (Mac...|\n",
      "|          8|1542262057796|     80| paid|   null|     null|       602|Portland-South Po...|\"Mozilla/5.0 (Mac...|\n",
      "|          9|1542262233796|     80| paid|   null|     null|       602|Portland-South Po...|\"Mozilla/5.0 (Mac...|\n",
      "|         10|1542262434796|     80| paid|   null|     null|       602|Portland-South Po...|\"Mozilla/5.0 (Mac...|\n",
      "|         11|1542262456796|     15| paid|   null|     null|       582|Chicago-Napervill...|\"Mozilla/5.0 (X11...|\n",
      "|         12|1542262679796|     80| paid|   null|     null|       602|Portland-South Po...|\"Mozilla/5.0 (Mac...|\n",
      "|         13|1542262728796|     15| paid|   null|     null|       582|Chicago-Napervill...|\"Mozilla/5.0 (X11...|\n",
      "|         14|1542262893796|     15| paid|   null|     null|       582|Chicago-Napervill...|\"Mozilla/5.0 (X11...|\n",
      "|         15|1542263158796|     15| paid|   null|     null|       582|Chicago-Napervill...|\"Mozilla/5.0 (X11...|\n",
      "|         16|1542263378796|     15| paid|   null|     null|       582|Chicago-Napervill...|\"Mozilla/5.0 (X11...|\n",
      "|         17|1542265716796|     26| free|   null|     null|       607|San Jose-Sunnyval...|\"Mozilla/5.0 (X11...|\n",
      "|         18|1542265929796|     26| free|   null|     null|       607|San Jose-Sunnyval...|\"Mozilla/5.0 (X11...|\n",
      "|         19|1542266927796|     49| paid|   null|     null|       606|San Francisco-Oak...|Mozilla/5.0 (Wind...|\n",
      "+-----------+-------------+-------+-----+-------+---------+----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "song_plays_table = spark.sql(\"\"\"\n",
    "    SELECT monotonically_increasing_id() as songplay_id, ts as start_time, userId as user_id, level, sv.song_id, av.artist_id, sessionId as session_id,\n",
    "    location, userAgent as user_agent FROM log_data ld\n",
    "    LEFT JOIN song_view sv ON ld.song=sv.title\n",
    "    LEFT JOIN artist_view av ON ld.artist=av.artist_name\"\"\")\n",
    "song_plays_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(output_data + \"songplays_table\",ignore_errors=True)\n",
    "song_plays_table.write.partitionBy(\"user_id\").parquet(output_data + \"songplays_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(\"./data/output/artists_table\", ignore_errors=True)\n",
    "shutil.rmtree(\"./data/output/songs_table\", ignore_errors=True)\n",
    "shutil.rmtree(\"./data/output/songplays_table\", ignore_errors=True)\n",
    "shutil.rmtree(\"./data/output/time_table\", ignore_errors=True)\n",
    "shutil.rmtree(\"./data/output/users_table\", ignore_errors=True)\n",
    "shutil.rmtree(\"./songs_table\", ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
